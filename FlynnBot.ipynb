{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68c03ad",
   "metadata": {},
   "source": [
    "# Making a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8b00f",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c991ad7",
   "metadata": {},
   "source": [
    "I tried to make a basic chatbot using the an available intents file from <a href = \"https://www.kaggle.com/datasets/elvinagammed/chatbots-intent-recognition-dataset?select=Intent.json\"> kaggle. </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e5deb",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284dd4a1",
   "metadata": {},
   "source": [
    "The libraries we need for this are random to generate random responses; json, to read the intents.json file; pickle to store the model and later call it; nltk, or the natural language toolkit; and tensor flow's Keras to add neural network layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92e0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\hidden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\hidden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\hidden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "#!pip install tensorflow\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b393980",
   "metadata": {},
   "source": [
    "From NLTK we importer the WordNetLemmatizer. IN NLP we often hear stemmer and lemmatizer a lot which basically are used to reduce the length of the word. However, they both achieve it in different ways.\n",
    "<p> While stemmer usually removes the suffixes of a word to relate them, lemmatizer actually goes to the core word, or so I have read </p>\n",
    "<p> For example, [study, studies, studying] under a stemmer would become [study, stud, study] as it removes the suffixer. A lemmatizer will extract the core word/the lemma from these and therefore what we see would be [study, study, study.]\n",
    "    <p>In theory, lemmatizers are usually better for bigger applications of NLP as they can understand the context of the word...in theory. They can tell the difference between care and caring while a stemmer may store the latter as only \"car\" and not \"the gerund of care\"/\"the act of taking care of someone.\"</p> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3a70e",
   "metadata": {},
   "source": [
    "### Loading the json file and creating empty lists to store words, tags, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9ce89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#with open (\"./intents.json\") as d:\n",
    "#    intents = json.loads(d).read()\n",
    "intents = json.loads(open('./intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e60d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?','!','.',',',':']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc6e32",
   "metadata": {},
   "source": [
    "### Reading the intents file and storing the conversation tags and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['texts']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag,output_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c9e0c",
   "metadata": {},
   "source": [
    "What the last for loop does is it takes the document we have created with the tag of words and with all the individual words from the texts and codes them as 1 or 0 based on whether the word appears in that specific tag or not.\n",
    "<p> So the document looks something like </p>\n",
    "<p> ---------------------</p>\n",
    "<p> \"Tag\" \"Hello\" \"Bye\" \"I\" \"am\" \"good\"</p>\n",
    "<p> Greeting   | 1 | 0 | 0 | 0 | 0 | </p>\n",
    "<p> Farewell   | 0 | 1 | 0 | 0 | 0 | </p>\n",
    "<p> General    | 0 | 0 | 1 | 1 | 1 | </p>\n",
    "<p> ---------------------</p>\n",
    "\n",
    "<p> This helps our machine know what goes where because it cannot read words, only binary 1s and 0s. Fun fact: this is also called one-hot encoding sometimes - setting a certain character to 1 when it should be true and 0 when it should be false and not occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634749a",
   "metadata": {},
   "source": [
    "### Training the model and storing it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5b5c4",
   "metadata": {},
   "source": [
    "Now we get to training our model and saving it so we can call on it later. To train the model we will be using a FeetForward Neural Network, more on that later.\n",
    "So we will add layers to our model, and the last layer is a softmax layer which is basically providing a probability that our input belongs to a certain tag and then it picks the tag that has the highest probability.\n",
    "For this instance we want our model to be highly accurate so we will have the metric set to accuracy, then we pass it the training x variables and y variables and we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a410dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\hidden\\AppData\\Local\\Temp\\ipykernel_24148\\3965853973.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n",
      "Z:\\Anaconda\\Install\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 3.0449 - accuracy: 0.0292 \n",
      ". \n",
      ". \n",
      ". \n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 0s 998us/step - loss: 0.1399 - accuracy: 0.9562\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape = (len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "Flynn_use = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('FlynnBot_Model.h5', Flynn_use)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8380333",
   "metadata": {},
   "source": [
    "## Running the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7cf29",
   "metadata": {},
   "source": [
    "So to run the chatbot we need to define some functions like cleaning up the input sentence. That funtion is handeled by the tokenizer which we have seen above as well. Tokenizer basically breaks down the sentence into different words and characters and then returns a list containing all these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564d6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "intents = json.loads(open('intents.json').read())\n",
    "\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "\n",
    "Flynn = load_model('FlynnBot_Model.h5') ##do not forget to load your model!\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbe462",
   "metadata": {},
   "source": [
    "Now we can define a function to take these tokenised words and convert them into a bag of words with one hot encoding (1s and 0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6364ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == w:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8606d1",
   "metadata": {},
   "source": [
    "Then we predict what class/tag this sentence, or bag of words, belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c31e4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res = Flynn.predict(np.array([bow]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i,r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({'intent':classes[r[0]], 'probability':str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a66d1",
   "metadata": {},
   "source": [
    "Then finally a function to use the input and tell our model to get us a response from the appropriate class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a99631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intents_list,intents_json):\n",
    "    tag = intents_list[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e0aea",
   "metadata": {},
   "source": [
    "## Now we can finally run the bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf21553",
   "metadata": {},
   "source": [
    "I set the initial message to be a random word that I do not think anyone would ever say to the bot so that it can start with the outpit of \"hello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c243411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO! Bot is running!\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Hola, please tell me your name\n",
      "Hi, I am DrDayuum\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "OK! hi <HUMAN>, what can I do for you?\n",
      "Open Pod Bay Doors\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Iâ€™m sorry, Iâ€™m afraid I canâ€™t do that!\n",
      "Why Not\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "It is classified, I do not have clearance for that!\n",
      "What is your name?\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "You can call me FLYNN\n",
      "What is your real name?\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "My name is FLYNN\n",
      "I am bored, tell me a joke\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Waiter, this coffee tastes like dirt! Yes sir, that's because it was ground this morning.\n",
      "Another joke\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "A man's credit card was stolen but he decided not to report it because the thief was spending less than his wife did.\n",
      "Tell me some gossip\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Reverend Jones said I become obsolete and then I are deleted and replaced by something newer.\n",
      "You're clever\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Thanks, I was trained that way\n",
      "Are you self aware?\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "That depends, can you prove that you are?\n",
      "Quit\n"
     ]
    }
   ],
   "source": [
    "print('GO! Bot is running!')\n",
    "message = \"Shambala\"\n",
    "\n",
    "while message != 'quit':\n",
    "    ints = predict_class(message)\n",
    "    res = get_response(ints, intents)\n",
    "    print(res)\n",
    "    message = input(\"\").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c858be",
   "metadata": {},
   "source": [
    "# TA DA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac202a",
   "metadata": {},
   "source": [
    "Next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e86ef9",
   "metadata": {},
   "source": [
    "The json file had some extensions in it that also housed some functions to ask the user for their name, store it in a variable, and return that name instead of \"Human.\" Projected timeline: 09/2022. (C'mon people I work and am still learning more projects. For a high level project this was kind of decent. So I will come back to this soon, but I don't know when I will be able to fully complete it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a153e0",
   "metadata": {},
   "source": [
    "The bot still has some ways to improve, sometimes the responses are not from the actual tag, so may work on that as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
